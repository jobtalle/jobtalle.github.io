<h2>Layered voxel rendering</h2>
<p>Three dimensional images can be stored using <em>voxels</em>. Voxels are effectively three dimensional pixels. Contrary to common 3D models that store scenes as large collections of triangular planes, voxels can encode volume as well. Additionally, their detail is equal for every part of the scene. The drawbacks however are numerous:</p>
<ul>
    <li>Voxels take up a lot of storage space. Storing colors and surface data of volume interiors that will likely never be rendered need to be stored regardless.</li>
    <li>Rendering voxels is slow. Rendering ten thousands of triangles is very doable on modern hardware, but a typical voxel scene has many more voxels than triangles. Also, voxels are often rendered as little cubes consisting of multiple triangles, which makes rendering even slower.</li>
    <li>Voxel models are not infinitely precise, so the angle of a surface is hard to determine. If one would zoom in on a 2D pixel image for example, no edge will have a smooth angle. The angle of three dimensional surfaces is however very important to calculate the interaction with light sources in a scene. Without accurate surface angles, shading will not work properly.</li>
</ul>
<figure title="A rendered scene">
    <img local src="img/island.jpg">
    <figcaption>Figure 1: A rendered voxel scene.</figcaption>
</figure>
<p>In this article, I propose a real-time voxel rendering method called <em>layered voxel rendering</em>, while trying to tackle these issues. Additionally, an interactive real-time javascript renderer is demonstrated using three different rendering methods. A procedural island generator is used to provide scenes for the renderers. Figure 1 shows a procedurally generated island rendered using the included renderer. The island generator itself is explained in the appendix.</p>
<p>The rendering method supports</p>
<ul>
    <li>real time voxel rendering at reasonable speed,</li>
    <li>a way to store scenes without storing every voxel in it,</li>
    <li>and a lighting model that does not require normals to be calculated while rendering.</li>
</ul>
<figure title="Voxel layers">
    <img local src="img/layers.png">
    <figcaption>Figure 2: Stacked images as voxel scene layers.</figcaption>
</figure>
<h2>Voxel layers</h2>
<p>The voxel scene that needs to be rendered is first split up in layers; for every vertical layer of voxels, a <em>slice</em> is created, containing all voxels at that layer. The result is a two dimensional image per layer. Every slice is simply an image. When rendering, these images are drawn from the bottom to the top. Figure 2 shows a schematic of a rendered scene, where layers are used to compose a three dimensional image.</p>
<p>Note that all images are scaled down vertically. This scale changes the camera pitch angle; scaling down further points the camera more towards the direction of the horizon. To rotate the scene, all images are rotated along the vertical axis before scaling is applied.</p>
<p></p>
<h2>Representing a scene</h2>
<p>The voxel scene will be represented using <em>primitive shapes</em>, and, for the sake of the island example, a heightmap shape to represent the terrain. It is clearer and simpler to populate a scene with shapes than voxels. This representation is also much more compact than storing every voxel in the scene. To be able to make the island shown in Figure 1, the following shapes are needed:</p>
<ul>
    <li>A heightmap to render the terrain.</li>
    <li>Cones to render the trees and roofs.</li>
    <li>Cylinders to render the huts.</li>
</ul>
<p>The trees in the scene are rendered using cones with a <em>voxel density</em> parameter; this makes the shapes not fully solid. The lower the density, the more voxels are omitted. This creates an effect that works well for plants and trees.</p>
<p>Each shape in the scene has a <em>bounding box</em>, which is a three dimensional box encompassing the shape. For every coordinate inside this box, the shape object will tell whether a voxel exists there. If a voxel exists, the object returns the voxel for that coordinate.</p>
<p>As mentioned in the first section, lighting and shading can be tricky when rendering voxels. To make it fast and accurate, I pre-shade the voxels; when a shape is sampled and when it returns a voxel, it returns a color with shading applied to it. To shade a voxel, the surface angle of that voxel is required. Shapes always give their voxels the surface normal <em>of the nearest surface</em>. Since it is possible to "peek between the layers" (see Figure 2), occluded voxels must still have the color and shading associated with their nearest surface area to prevent ugly rendering artifacs, since they may be partially visible.</p>
<figure title="A voxel shape">
    <img local src="img/shape.png">
    <figcaption>Figure 3: Translating a shape to voxel layers.</figcaption>
</figure>
<h2>Translating scenes into layers</h2>
<p>Once shapes are properly defined, creating layers is straightforward:</p>
<ol>
    <li>A region to "voxelize" is chosen. Parts of the scene may be omitted, or a scene can be broken up into multiple blocks of layers to make sure not everything needs to be rendered all the time.</li>
    <li>A number of layers (which are just images) equal to the height of the render area is created.</li>
    <li>For every pixel on every image, the voxel for that coordinate is queried and rendered to its layer.</li>
</ol>
<p>This process is shown in Figure 3, where a sphere shape is translated to voxel layers. Once the layers are created, they can be sent to the renderer to produce the complete and interactive voxel image.</p>
<h2>An interactive renderer</h2>

<h2>Rendering methods</h2>
<p>Rendering transformed images is pretty easy, but there are different ways to go about it. I implemented three renderers to compare their performance.</p>
<ol>
    <li>Since I render my layers on canvases in javascript, a <em>canvas renderer</em> is the most obvious choice. This renderer is quite performant compared to the others, since canvas rendering is GPU accelerated on modern browsers.</li>
    <li>A <em>WebGL renderer</em> is also included. I'm using the <a href="https://github.com/jobtalle/myr.js" target="_blank">myr.js</a> library, which uses WebGL 2. Converting canvas pixels to WebGL textures takes some time, because all data needs to be copied over. Once the renderer runs, I don't see much difference in performance compared to the canvas renderer. Note that this option only appears in the example above if your browser supports WebGL 2.</li>
    <li>Finally, I implemented a <em>CSS renderer</em>. Here, every layer is instantiated as an HTML element and transformed using CSS transforms. On most browsers, this method is by far the slowest. To my surprise however, it was very performant on Microsoft Edge, in some instances even faster than both the canvas and WebGL renderers on other browsers. I cannot explain this.</li>
</ol>
<p>Because all the renderers need to do is rendering transformed images, they all produce the same image quality. When implementing this technique, the method that runs best on the target platform should be chosen.</p>
<h2>Conclusion</h2>
<p>The proposed method can be extended in a number of ways:</p>
<ul>
    <li>Scenes can be split up into <em>chunks</em>. Currently, a lot of empty air and invisible underground voxels are rendered (but never seen). Splitting up the scene into chunks and omitting chunks that do not require rendering both improves performance (while also reducing the memory footprint) and allows invisible parts of the scene to be rendered only when they come into view. Additionally, the scene size is no longer restricted, since shapes are only queried when rendering chunks that contain them.</li>
    <li>Perspective transformation can be added. For many 3D applications, perspective rendering is preferred over isometric rendering.</li>
    <li>A level of detail criterion can be added. Currently, all voxels are rendered at the maximum resolution. It would however be possible to render half or quarter resolution layers that can be displayed for distant parts of the scene until the camera comes closer. Note that this only makes sense when perspective is used, since distant objects don't become smaller in isometric renderers.</li>
</ul>
<p>The proposed method has some drawbacks too:</p>
<ul>
    <li>The camera pitch angle must be within a certain range. If the camera is pitched horizontally enough, you can see too far under the vertical layers. Voxels that should not be seen are then revealed, creating messy images.</li>
    <li>Layers need to be <em>pre-rendered</em>. Another rendering pass is required for dynamic objects. Rendering large moving objects could be slow. This performance hit can be avoided by using a <em>depth buffer</em> while rendering the scene; after the chunks have been rendered, dynamic objects can then be inserted into the scene as traditional 3D models using the populated depth buffer.</li>
    <li>Rendering is still quite slow compared to traditional 3D rendering. There must be a good reason for using voxels to make this method interesting. Once the layers have been rendered though, this method can be implemented in almost every framework and on almost every device, no 3D hardware or software is required (the CSS renderer shows it even works in plain HTML for example).</li>
</ul>
<h2>Appendix: procedurally generating islands</h2>